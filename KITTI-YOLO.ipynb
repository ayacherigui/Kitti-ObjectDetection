{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=data.yaml, epochs=50, patience=50, batch=8, imgsz=1244, save=True, cache=False, device=, workers=8, project=None, name=yolov8n_custom, exist_ok=False, pretrained=False, optimizer=SGD, verbose=False, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs/detect/yolov8n_custom15\n",
      "Overriding model.yaml nc=80 with nc=9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753067  ultralytics.nn.modules.Detect                [9, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3012603 parameters, 3012587 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "WARNING ‚ö†Ô∏è --img-size [1244] must be multiple of max stride 32, updating to [1248]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/aya/Kitti-ObjectDetection/kitty/train/labels.cache... 5313 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5313/5313 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/aya/Kitti-ObjectDetection/kitty/valid/labels.cache... 1513 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1513/1513 [00:00<?, ?it/s]\n",
      "Image sizes 1248 train, 1248 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/yolov8n_custom15\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50      9.84G      1.404      2.071      1.203          0       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:26<00:00,  2.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  6.96it/s]\n",
      "                   all       1513      10350      0.503       0.39      0.373      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50      6.09G      1.296      1.353      1.156         13       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:23<00:00,  2.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  6.85it/s]\n",
      "                   all       1513      10350      0.514      0.486      0.478      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50      6.09G      1.283      1.259      1.153          9       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:20<00:00,  2.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.16it/s]\n",
      "                   all       1513      10350      0.531      0.533      0.529      0.321\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50      6.09G      1.271      1.199      1.153         31       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:21<00:00,  2.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:14<00:00,  6.73it/s]\n",
      "                   all       1513      10350      0.532      0.539      0.525      0.326\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50      6.09G      1.243      1.117      1.138         17       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:20<00:00,  2.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.28it/s]\n",
      "                   all       1513      10350      0.626      0.577      0.609      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50      6.09G      1.207      1.049      1.126         19       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:16<00:00,  2.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.36it/s]\n",
      "                   all       1513      10350      0.606      0.627       0.65      0.407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50      6.09G      1.184     0.9959      1.113         11       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:02<00:00,  2.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.70it/s]\n",
      "                   all       1513      10350      0.687      0.616       0.67      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50      6.09G       1.16     0.9538      1.105          2       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:00<00:00,  2.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.67it/s]\n",
      "                   all       1513      10350      0.649       0.64      0.678      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50      6.09G      1.144      0.926      1.095         16       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:01<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.66it/s]\n",
      "                   all       1513      10350      0.703      0.629        0.7      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50      6.09G      1.126     0.9004       1.09         15       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:01<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.61it/s]\n",
      "                   all       1513      10350      0.691      0.663      0.696      0.456\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50      6.09G      1.112      0.877      1.082          4       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:01<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.74it/s]\n",
      "                   all       1513      10350      0.737      0.675      0.735      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50      6.09G      1.093     0.8474      1.071         14       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:00<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.62it/s]\n",
      "                   all       1513      10350      0.749      0.693      0.738      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50      6.09G       1.08     0.8267      1.065         11       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:00<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.64it/s]\n",
      "                   all       1513      10350      0.751      0.712      0.755      0.504\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50      6.09G       1.07     0.8126      1.061         16       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:00<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.73it/s]\n",
      "                   all       1513      10350      0.778      0.696      0.764      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50      6.09G      1.063     0.8054      1.056         10       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:01<00:00,  2.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.62it/s]\n",
      "                   all       1513      10350      0.737      0.713      0.764      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50      6.09G      1.048     0.7883      1.053         30       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:12<00:00,  2.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.39it/s]\n",
      "                   all       1513      10350      0.798      0.717      0.774      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50      6.09G      1.038     0.7749      1.047         21       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.28it/s]\n",
      "                   all       1513      10350      0.769      0.734       0.79      0.524\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50      6.09G       1.03     0.7634      1.039         13       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.31it/s]\n",
      "                   all       1513      10350      0.794      0.741      0.779      0.529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50      6.09G      1.016      0.752      1.034         28       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.35it/s]\n",
      "                   all       1513      10350       0.76      0.744      0.792      0.541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50      6.09G      1.006     0.7363       1.03         26       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.27it/s]\n",
      "                   all       1513      10350      0.731       0.76      0.797      0.539\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50      6.09G      1.002      0.731      1.027         22       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:16<00:00,  2.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.40it/s]\n",
      "                   all       1513      10350      0.778      0.756      0.812      0.555\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50       6.1G     0.9948     0.7216      1.022          8       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350      0.797       0.74      0.799      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50       6.1G     0.9761     0.7058      1.012         12       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:16<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.24it/s]\n",
      "                   all       1513      10350      0.787       0.77      0.817      0.566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50       6.1G     0.9731     0.7035      1.017          7       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.27it/s]\n",
      "                   all       1513      10350      0.814       0.75       0.82      0.565\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50       6.1G     0.9674     0.6956       1.01         12       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.33it/s]\n",
      "                   all       1513      10350      0.796      0.767      0.827      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50       6.1G     0.9615     0.6868      1.009          9       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.35it/s]\n",
      "                   all       1513      10350      0.769      0.788      0.822      0.576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50       6.1G     0.9479     0.6826      1.003         14       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.28it/s]\n",
      "                   all       1513      10350      0.802      0.772      0.825      0.578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50       6.1G     0.9458     0.6737      1.002          7       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:16<00:00,  2.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.37it/s]\n",
      "                   all       1513      10350      0.798      0.776      0.827      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50       6.1G     0.9352     0.6614     0.9973         15       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350      0.808      0.787       0.83      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50       6.1G     0.9193     0.6504     0.9911          7       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.34it/s]\n",
      "                   all       1513      10350      0.804       0.78      0.837      0.594\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50       6.1G     0.9237     0.6528     0.9909         24       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.25it/s]\n",
      "                   all       1513      10350      0.813      0.795      0.847      0.598\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50       6.1G     0.9153     0.6434     0.9881          7       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.22it/s]\n",
      "                   all       1513      10350      0.803      0.805      0.844        0.6\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50       6.1G     0.9117     0.6393     0.9837         20       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.39it/s]\n",
      "                   all       1513      10350      0.837       0.78      0.841      0.597\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50       6.1G     0.9052     0.6351     0.9852         17       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:16<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350       0.83        0.8      0.852      0.607\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50       6.1G     0.8957      0.624     0.9794          8       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.22it/s]\n",
      "                   all       1513      10350      0.834      0.791      0.853      0.611\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50       6.1G     0.8944      0.627     0.9773         19       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.34it/s]\n",
      "                   all       1513      10350      0.836      0.794      0.857      0.615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50       6.1G     0.8848     0.6108     0.9751         23       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:15<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350       0.85      0.791      0.857      0.614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50       6.1G     0.8796     0.6071     0.9726          6       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.23it/s]\n",
      "                   all       1513      10350      0.851      0.797      0.856      0.616\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50       6.1G     0.8695     0.6014     0.9673         30       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:14<00:00,  2.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.37it/s]\n",
      "                   all       1513      10350      0.824      0.812      0.862      0.629\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50       6.1G      0.863     0.5959     0.9662          7       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:16<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.18it/s]\n",
      "                   all       1513      10350      0.857      0.805      0.868      0.631\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50       6.1G     0.8702     0.5895     0.9652          9       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.27it/s]\n",
      "                   all       1513      10350       0.84      0.811       0.86      0.622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50       6.1G     0.8579     0.5801     0.9607          2       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:05<00:00,  2.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.37it/s]\n",
      "                   all       1513      10350      0.828      0.797      0.854      0.625\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50       6.1G     0.8491     0.5719     0.9569          5       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.23it/s]\n",
      "                   all       1513      10350      0.818      0.811      0.859      0.624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50       6.1G     0.8365     0.5613      0.951         12       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350      0.847      0.808       0.86      0.628\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50       6.1G     0.8295     0.5553     0.9507          5       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.29it/s]\n",
      "                   all       1513      10350      0.845      0.818      0.867      0.635\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50       6.1G     0.8229     0.5488     0.9431         15       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:05<00:00,  2.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:12<00:00,  7.32it/s]\n",
      "                   all       1513      10350      0.868        0.8      0.862      0.634\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50       6.1G      0.816     0.5424     0.9423          7       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:06<00:00,  2.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.24it/s]\n",
      "                   all       1513      10350      0.866      0.811       0.87      0.642\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50       6.1G     0.8074     0.5373     0.9395          6       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350      0.835      0.828       0.87      0.644\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50       6.1G     0.7985     0.5311     0.9372          6       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:13<00:00,  7.30it/s]\n",
      "                   all       1513      10350      0.864      0.816      0.873      0.648\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50       6.1G     0.7893     0.5215     0.9323          5       1248: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [04:04<00:00,  2.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:15<00:00,  6.21it/s]\n",
      "                   all       1513      10350      0.856      0.822      0.871      0.646\n",
      "\n",
      "50 epochs completed in 3.690 hours.\n",
      "Optimizer stripped from runs/detect/yolov8n_custom15/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/yolov8n_custom15/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/yolov8n_custom15/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "Model summary (fused): 168 layers, 3007403 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:14<00:00,  6.73it/s]\n",
      "                   all       1513      10350      0.863      0.816      0.873      0.648\n",
      "               Cyclist       1513        341      0.913      0.815      0.891      0.617\n",
      "              DontCare       1513       2252      0.635      0.315      0.412      0.158\n",
      "                  Misc       1513        216       0.88       0.87       0.93      0.696\n",
      "        Person_sitting       1513         42      0.802      0.833      0.894      0.588\n",
      "                  Tram       1513         95      0.867      0.958      0.973      0.794\n",
      "                 Truck       1513        225       0.94       0.96      0.979      0.847\n",
      "                   Van       1513        581      0.931      0.924      0.963      0.798\n",
      "                   car       1513       5667      0.942      0.947      0.979      0.833\n",
      "                person       1513        931       0.86      0.724       0.84      0.497\n",
      "Speed: 0.4ms pre-process, 3.6ms inference, 0.0ms loss, 0.6ms post-process per image\n",
      "Results saved to \u001b[1mruns/detect/yolov8n_custom15\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 22:58:37,364 - clearml.Task - INFO - Completed model upload to https://files.clear.ml/YOLOv8/yolov8n_custom.40d6ded8fdf64ee6b1c5320e391c8443/models/best.pt\n",
      "2023-03-16 22:58:41,756 - clearml.Task - INFO - Completed model upload to https://files.clear.ml/YOLOv8/yolov8n_custom.40d6ded8fdf64ee6b1c5320e391c8443/models/best.pt\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    " \n",
    "# Training.\n",
    "results = model.train(\n",
    "   data='data.yaml',\n",
    "   imgsz=1244,\n",
    "   epochs=50,\n",
    "   batch=8,\n",
    "   name='yolov8n_custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "    handler._at_fork_reinit()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "    self.lock._at_fork_reinit()\n",
      "AttributeError: 'NoneType' object has no attribute '_at_fork_reinit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "Model summary (fused): 168 layers, 3007403 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/aya/Kitti-ObjectDetection/kitty/valid/labels.cache... 1513 i\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       1513      10350      0.863      0.817      0.874      0.648\n",
      "               Cyclist       1513        341      0.907      0.815       0.89      0.616\n",
      "              DontCare       1513       2252      0.638      0.314      0.413      0.158\n",
      "                  Misc       1513        216      0.882       0.87       0.93      0.699\n",
      "        Person_sitting       1513         42      0.801      0.833      0.895      0.588\n",
      "                  Tram       1513         95      0.867      0.958      0.971      0.794\n",
      "                 Truck       1513        225      0.941       0.96      0.979      0.846\n",
      "                   Van       1513        581      0.931      0.924      0.963      0.801\n",
      "                   car       1513       5667      0.942      0.946      0.979      0.834\n",
      "                person       1513        931      0.862      0.727      0.844      0.495\n",
      "Speed: 0.5ms pre-process, 2.8ms inference, 0.0ms loss, 0.7ms post-process per image\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=val model=yolov8n_costum.pt data=data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/aya/Kitti-ObjectDetection/kitty/valid/labels.cache... 1513 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1513/1513 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/190 [00:00<?, ?it/s]Exception ignored in: <function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in:   File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "<function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>\n",
      "    handler._at_fork_reinit()Exception ignored in: Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "<function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "    handler._at_fork_reinit()    \n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "self.lock._at_fork_reinit()  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "\n",
      "<function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>AttributeError\n",
      "        : self.lock._at_fork_reinit()handler._at_fork_reinit()'NoneType' object has no attribute '_at_fork_reinit'\n",
      "\n",
      "\n",
      "Exception ignored in: AttributeErrorTraceback (most recent call last):\n",
      ":   File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "<function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>'NoneType' object has no attribute '_at_fork_reinit'\n",
      "\n",
      "        handler._at_fork_reinit()self.lock._at_fork_reinit()\n",
      "Exception ignored in: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "AttributeError<function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>    : \n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "self.lock._at_fork_reinit()'NoneType' object has no attribute '_at_fork_reinit'\n",
      "    \n",
      "AttributeErrorhandler._at_fork_reinit()Traceback (most recent call last):\n",
      ": \n",
      "'NoneType' object has no attribute '_at_fork_reinit'  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "    \n",
      "    self.lock._at_fork_reinit()handler._at_fork_reinit()\n",
      "\n",
      "AttributeError  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      ":     'NoneType' object has no attribute '_at_fork_reinit'self.lock._at_fork_reinit()\n",
      "\n",
      "AttributeError: 'NoneType' object has no attribute '_at_fork_reinit'\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [00:14<00:00, 13.16it/s]\n",
      "                   all       1513      10350      0.863      0.817      0.874      0.648\n",
      "               Cyclist       1513        341      0.907      0.815       0.89      0.616\n",
      "              DontCare       1513       2252      0.638      0.314      0.413      0.158\n",
      "                  Misc       1513        216      0.882       0.87       0.93      0.699\n",
      "        Person_sitting       1513         42      0.801      0.833      0.895      0.588\n",
      "                  Tram       1513         95      0.867      0.958      0.971      0.794\n",
      "                 Truck       1513        225      0.941       0.96      0.979      0.846\n",
      "                   Van       1513        581      0.931      0.924      0.963      0.801\n",
      "                   car       1513       5667      0.942      0.946      0.979      0.834\n",
      "                person       1513        931      0.862      0.727      0.844      0.495\n",
      "Speed: 0.4ms pre-process, 3.4ms inference, 0.0ms loss, 0.7ms post-process per image\n"
     ]
    }
   ],
   "source": [
    "#!yolo task=detect mode=val model=\"/runs/detect/yolov8n_custom6/weights/best.pt\" data=data.yaml\n",
    "results = model.val()  # evaluate model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "    handler._at_fork_reinit()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "    self.lock._at_fork_reinit()\n",
      "AttributeError: 'NoneType' object has no attribute '_at_fork_reinit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "image 1/1 /home/aya/Kitti-ObjectDetection/kitty/train/images/000036_png.rf.04ff80c138765044b69cb8df5d4078f4.jpg: 224x640 5 cars, 1 traffic light, 10.3ms\n",
      "Speed: 0.3ms pre-process, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Before yolo\n",
    "!yolo task=detect mode=predict model=yolov8n.pt source=\"kitty/test/images/000036_png.rf.04ff80c138765044b69cb8df5d4078f4.jpg\" save=True conf=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _after_at_fork_child_reinit_locks at 0x7fa87bb39ab0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 256, in _after_at_fork_child_reinit_locks\n",
      "    handler._at_fork_reinit()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 910, in _at_fork_reinit\n",
      "    self.lock._at_fork_reinit()\n",
      "AttributeError: 'NoneType' object has no attribute '_at_fork_reinit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "Model summary (fused): 168 layers, 3007403 parameters, 0 gradients, 8.1 GFLOPs\n",
      "image 1/1 /home/aya/Kitti-ObjectDetection/kitty/train/images/000036_png.rf.04ff80c138765044b69cb8df5d4078f4.jpg: 384x1248 7 cars, 9.1ms\n",
      "Speed: 0.5ms pre-process, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1248, 1248)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Try yolov8 on images\n",
    "!yolo task=detect mode=predict model=yolov8n_costum.pt source=\"kitty/test/images/000036_png.rf.04ff80c138765044b69cb8df5d4078f4.jpg\" save=True conf=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\n",
      "Model summary (fused): 168 layers, 3007403 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /home/aya/Desktop/Kitti-ObjectDetection/runs/detect/yolov8n_custom15/weights/best.pt with output shape (1, 13, 31941) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 0.5s, saved as /home/aya/Desktop/Kitti-ObjectDetection/runs/detect/yolov8n_custom15/weights/best.onnx (12.1 MB)\n",
      "\n",
      "Export complete (5.9s)\n",
      "Results saved to \u001b[1m/home/aya/Desktop/Kitti-ObjectDetection/runs/detect/yolov8n_custom15/weights\u001b[0m\n",
      "Predict:         yolo task=detect mode=predict model=/home/aya/Desktop/Kitti-ObjectDetection/runs/detect/yolov8n_custom15/weights/best.onnx -WARNING ‚ö†Ô∏è not yet supported for YOLOv8 exported models\n",
      "Validate:        yolo task=detect mode=val model=/home/aya/Desktop/Kitti-ObjectDetection/runs/detect/yolov8n_custom15/weights/best.onnx -WARNING ‚ö†Ô∏è not yet supported for YOLOv8 exported models\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/home/aya/Desktop/Kitti-ObjectDetection/runs/detect/yolov8n_custom15/weights/best.pt')\n",
    "# Export the model in onnx format\n",
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement core (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for core\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements the boundary attack `BoundaryAttack`. This is a black-box attack which only requires class\n",
    "predictions.\n",
    "| Paper link: https://arxiv.org/abs/1712.04248\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "from typing import Optional, Tuple, TYPE_CHECKING\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from art.attacks.attack import EvasionAttack\n",
    "from art.config import ART_NUMPY_DTYPE\n",
    "from art.estimators.estimator import BaseEstimator\n",
    "from art.estimators.classification.classifier import ClassifierMixin\n",
    "from art.utils import compute_success, to_categorical, check_and_transform_label_format, get_labels_np_array\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from art.utils import CLASSIFIER_TYPE\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BoundaryAttack(EvasionAttack):\n",
    "    \"\"\"\n",
    "    Implementation of the boundary attack from Brendel et al. (2018). This is a powerful black-box attack that\n",
    "    only requires final class prediction.\n",
    "    | Paper link: https://arxiv.org/abs/1712.04248\n",
    "    \"\"\"\n",
    "\n",
    "    attack_params = EvasionAttack.attack_params + [\n",
    "        \"targeted\",\n",
    "        \"delta\",\n",
    "        \"epsilon\",\n",
    "        \"step_adapt\",\n",
    "        \"max_iter\",\n",
    "        \"num_trial\",\n",
    "        \"sample_size\",\n",
    "        \"init_size\",\n",
    "        \"batch_size\",\n",
    "        \"verbose\",\n",
    "    ]\n",
    "\n",
    "    _estimator_requirements = (BaseEstimator, ClassifierMixin)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: \"CLASSIFIER_TYPE\",\n",
    "        batch_size: int = 64,\n",
    "        targeted: bool = True,\n",
    "        delta: float = 0.01,\n",
    "        epsilon: float = 0.01,\n",
    "        step_adapt: float = 0.667,\n",
    "        max_iter: int = 5000,\n",
    "        num_trial: int = 25,\n",
    "        sample_size: int = 20,\n",
    "        init_size: int = 100,\n",
    "        min_epsilon: float = 0.0,\n",
    "        verbose: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create a boundary attack instance.\n",
    "        :param estimator: A trained classifier.\n",
    "        :param batch_size: The size of the batch used by the estimator during inference.\n",
    "        :param targeted: Should the attack target one specific class.\n",
    "        :param delta: Initial step size for the orthogonal step.\n",
    "        :param epsilon: Initial step size for the step towards the target.\n",
    "        :param step_adapt: Factor by which the step sizes are multiplied or divided, must be in the range (0, 1).\n",
    "        :param max_iter: Maximum number of iterations.\n",
    "        :param num_trial: Maximum number of trials per iteration.\n",
    "        :param sample_size: Number of samples per trial.\n",
    "        :param init_size: Maximum number of trials for initial generation of adversarial examples.\n",
    "        :param min_epsilon: Stop attack if perturbation is smaller than `min_epsilon`.\n",
    "        :param verbose: Show progress bars.\n",
    "        \"\"\"\n",
    "        super().__init__(estimator=estimator)\n",
    "\n",
    "        self._targeted = targeted\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        self.step_adapt = step_adapt\n",
    "        self.max_iter = max_iter\n",
    "        self.num_trial = num_trial\n",
    "        self.sample_size = sample_size\n",
    "        self.init_size = init_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self._check_params()\n",
    "\n",
    "        self.curr_adv: Optional[np.ndarray] = None\n",
    "\n",
    "    def generate(self, x: np.ndarray, y: Optional[np.ndarray] = None, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate adversarial samples and return them in an array.\n",
    "        :param x: An array with the original inputs to be attacked.\n",
    "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
    "                  (nb_samples,). If `self.targeted` is true, then `y` represents the target labels.\n",
    "        :param x_adv_init: Initial array to act as initial adversarial examples. Same shape as `x`.\n",
    "        :type x_adv_init: `np.ndarray`\n",
    "        :return: An array holding the adversarial examples.\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            # Throw error if attack is targeted, but no targets are provided\n",
    "            if self.targeted:  # pragma: no cover\n",
    "                raise ValueError(\"Target labels `y` need to be provided for a targeted attack.\")\n",
    "\n",
    "            # Use model predictions as correct outputs\n",
    "            y = get_labels_np_array(self.estimator.predict(x, batch_size=self.batch_size))  # type: ignore\n",
    "\n",
    "        y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes, return_one_hot=False)\n",
    "\n",
    "        # Get clip_min and clip_max from the classifier or infer them from data\n",
    "        if self.estimator.clip_values is not None:\n",
    "            clip_min, clip_max = self.estimator.clip_values\n",
    "        else:\n",
    "            clip_min, clip_max = np.min(x), np.max(x)\n",
    "\n",
    "        # Prediction from the original images\n",
    "        preds = np.argmax(self.estimator.predict(x, batch_size=self.batch_size), axis=1)\n",
    "\n",
    "        # Prediction from the initial adversarial examples if not None\n",
    "        x_adv_init = kwargs.get(\"x_adv_init\")\n",
    "\n",
    "        if x_adv_init is not None:\n",
    "            init_preds = np.argmax(self.estimator.predict(x_adv_init, batch_size=self.batch_size), axis=1)\n",
    "        else:\n",
    "            init_preds = [None] * len(x)\n",
    "            x_adv_init = [None] * len(x)\n",
    "\n",
    "        # Assert that, if attack is targeted, y is provided\n",
    "        if self.targeted and y is None:  # pragma: no cover\n",
    "            raise ValueError(\"Target labels `y` need to be provided for a targeted attack.\")\n",
    "\n",
    "        # Some initial setups\n",
    "        x_adv = x.astype(ART_NUMPY_DTYPE)\n",
    "\n",
    "        # Generate the adversarial samples\n",
    "        for ind, val in enumerate(tqdm(x_adv, desc=\"Boundary attack\", disable=not self.verbose)):\n",
    "            if self.targeted:\n",
    "                x_adv[ind] = self._perturb(\n",
    "                    x=val,\n",
    "                    y=y[ind],\n",
    "                    y_p=preds[ind],\n",
    "                    init_pred=init_preds[ind],\n",
    "                    adv_init=x_adv_init[ind],\n",
    "                    clip_min=clip_min,\n",
    "                    clip_max=clip_max,\n",
    "                )\n",
    "            else:\n",
    "                x_adv[ind] = self._perturb(\n",
    "                    x=val,\n",
    "                    y=-1,\n",
    "                    y_p=preds[ind],\n",
    "                    init_pred=init_preds[ind],\n",
    "                    adv_init=x_adv_init[ind],\n",
    "                    clip_min=clip_min,\n",
    "                    clip_max=clip_max,\n",
    "                )\n",
    "\n",
    "        y = to_categorical(y, self.estimator.nb_classes)\n",
    "\n",
    "        logger.info(\n",
    "            \"Success rate of Boundary attack: %.2f%%\",\n",
    "            100 * compute_success(self.estimator, x, y, x_adv, self.targeted, batch_size=self.batch_size),\n",
    "        )\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def _perturb(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: int,\n",
    "        y_p: int,\n",
    "        init_pred: int,\n",
    "        adv_init: np.ndarray,\n",
    "        clip_min: float,\n",
    "        clip_max: float,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Internal attack function for one example.\n",
    "        :param x: An array with one original input to be attacked.\n",
    "        :param y: If `self.targeted` is true, then `y` represents the target label.\n",
    "        :param y_p: The predicted label of x.\n",
    "        :param init_pred: The predicted label of the initial image.\n",
    "        :param adv_init: Initial array to act as an initial adversarial example.\n",
    "        :param clip_min: Minimum value of an example.\n",
    "        :param clip_max: Maximum value of an example.\n",
    "        :return: An adversarial example.\n",
    "        \"\"\"\n",
    "        # First, create an initial adversarial sample\n",
    "        initial_sample = self._init_sample(x, y, y_p, init_pred, adv_init, clip_min, clip_max)\n",
    "\n",
    "        # If an initial adversarial example is not found, then return the original image\n",
    "        if initial_sample is None:\n",
    "            return x\n",
    "\n",
    "        # If an initial adversarial example found, then go with boundary attack\n",
    "        x_adv = self._attack(\n",
    "            initial_sample[0],\n",
    "            x,\n",
    "            y_p,\n",
    "            initial_sample[1],\n",
    "            self.delta,\n",
    "            self.epsilon,\n",
    "            clip_min,\n",
    "            clip_max,\n",
    "        )\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def _attack(\n",
    "        self,\n",
    "        initial_sample: np.ndarray,\n",
    "        original_sample: np.ndarray,\n",
    "        y_p: int,\n",
    "        target: int,\n",
    "        initial_delta: float,\n",
    "        initial_epsilon: float,\n",
    "        clip_min: float,\n",
    "        clip_max: float,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Main function for the boundary attack.\n",
    "        :param initial_sample: An initial adversarial example.\n",
    "        :param original_sample: The original input.\n",
    "        :param y_p: The predicted label of the original input.\n",
    "        :param target: The target label.\n",
    "        :param initial_delta: Initial step size for the orthogonal step.\n",
    "        :param initial_epsilon: Initial step size for the step towards the target.\n",
    "        :param clip_min: Minimum value of an example.\n",
    "        :param clip_max: Maximum value of an example.\n",
    "        :return: an adversarial example.\n",
    "        \"\"\"\n",
    "        # Get initialization for some variables\n",
    "        x_adv = initial_sample\n",
    "        self.curr_delta = initial_delta\n",
    "        self.curr_epsilon = initial_epsilon\n",
    "\n",
    "        self.curr_adv = x_adv\n",
    "\n",
    "        # Main loop to wander around the boundary\n",
    "        for _ in trange(self.max_iter, desc=\"Boundary attack - iterations\", disable=not self.verbose):\n",
    "            # Trust region method to adjust delta\n",
    "            for _ in range(self.num_trial):\n",
    "                potential_advs = []\n",
    "                for _ in range(self.sample_size):\n",
    "                    potential_adv = x_adv + self._orthogonal_perturb(self.curr_delta, x_adv, original_sample)\n",
    "                    potential_adv = np.clip(potential_adv, clip_min, clip_max)\n",
    "                    potential_advs.append(potential_adv)\n",
    "\n",
    "                preds = np.argmax(\n",
    "                    self.estimator.predict(np.array(potential_advs), batch_size=self.batch_size),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "                if self.targeted:\n",
    "                    satisfied = preds == target\n",
    "                else:\n",
    "                    satisfied = preds != y_p\n",
    "\n",
    "                delta_ratio = np.mean(satisfied)\n",
    "\n",
    "                if delta_ratio < 0.2:\n",
    "                    self.curr_delta *= self.step_adapt\n",
    "                elif delta_ratio > 0.5:\n",
    "                    self.curr_delta /= self.step_adapt\n",
    "\n",
    "                if delta_ratio > 0:\n",
    "                    x_advs = np.array(potential_advs)[np.where(satisfied)[0]]\n",
    "                    break\n",
    "            else:  # pragma: no cover\n",
    "                logger.warning(\"Adversarial example found but not optimal.\")\n",
    "                return x_adv\n",
    "\n",
    "            # Trust region method to adjust epsilon\n",
    "            for _ in range(self.num_trial):\n",
    "                perturb = np.repeat(np.array([original_sample]), len(x_advs), axis=0) - x_advs\n",
    "                perturb *= self.curr_epsilon\n",
    "                potential_advs = x_advs + perturb\n",
    "                potential_advs = np.clip(potential_advs, clip_min, clip_max)\n",
    "                preds = np.argmax(\n",
    "                    self.estimator.predict(potential_advs, batch_size=self.batch_size),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "                if self.targeted:\n",
    "                    satisfied = preds == target\n",
    "                else:\n",
    "                    satisfied = preds != y_p\n",
    "\n",
    "                epsilon_ratio = np.mean(satisfied)\n",
    "\n",
    "                if epsilon_ratio < 0.2:\n",
    "                    self.curr_epsilon *= self.step_adapt\n",
    "                elif epsilon_ratio > 0.5:\n",
    "                    self.curr_epsilon /= self.step_adapt\n",
    "\n",
    "                if epsilon_ratio > 0:\n",
    "                    x_adv = self._best_adv(original_sample, potential_advs[np.where(satisfied)[0]])\n",
    "                    self.curr_adv = x_adv\n",
    "                    break\n",
    "            else:  # pragma: no cover\n",
    "                logger.warning(\"Adversarial example found but not optimal.\")\n",
    "                return self._best_adv(original_sample, x_advs)\n",
    "\n",
    "            if self.curr_epsilon < self.min_epsilon:\n",
    "                return x_adv\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def _orthogonal_perturb(self, delta: float, current_sample: np.ndarray, original_sample: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create an orthogonal perturbation.\n",
    "        :param delta: Initial step size for the orthogonal step.\n",
    "        :param current_sample: Current adversarial example.\n",
    "        :param original_sample: The original input.\n",
    "        :return: a possible perturbation.\n",
    "        \"\"\"\n",
    "        # Generate perturbation randomly\n",
    "        perturb = np.random.randn(*self.estimator.input_shape).astype(ART_NUMPY_DTYPE)\n",
    "\n",
    "        # Rescale the perturbation\n",
    "        perturb /= np.linalg.norm(perturb)\n",
    "        perturb *= delta * np.linalg.norm(original_sample - current_sample)\n",
    "\n",
    "        # Project the perturbation onto sphere\n",
    "        direction = original_sample - current_sample\n",
    "\n",
    "        direction_flat = direction.flatten()\n",
    "        perturb_flat = perturb.flatten()\n",
    "\n",
    "        direction_flat /= np.linalg.norm(direction_flat)\n",
    "        perturb_flat -= np.dot(perturb_flat, direction_flat.T) * direction_flat\n",
    "        perturb = perturb_flat.reshape(self.estimator.input_shape)\n",
    "\n",
    "        hypotenuse = np.sqrt(1 + delta ** 2)\n",
    "        perturb = ((1 - hypotenuse) * (current_sample - original_sample) + perturb) / hypotenuse\n",
    "        return perturb\n",
    "\n",
    "    def _init_sample(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: int,\n",
    "        y_p: int,\n",
    "        init_pred: int,\n",
    "        adv_init: np.ndarray,\n",
    "        clip_min: float,\n",
    "        clip_max: float,\n",
    "    ) -> Optional[Tuple[np.ndarray, int]]:\n",
    "        \"\"\"\n",
    "        Find initial adversarial example for the attack.\n",
    "        :param x: An array with one original input to be attacked.\n",
    "        :param y: If `self.targeted` is true, then `y` represents the target label.\n",
    "        :param y_p: The predicted label of x.\n",
    "        :param init_pred: The predicted label of the initial image.\n",
    "        :param adv_init: Initial array to act as an initial adversarial example.\n",
    "        :param clip_min: Minimum value of an example.\n",
    "        :param clip_max: Maximum value of an example.\n",
    "        :return: an adversarial example.\n",
    "        \"\"\"\n",
    "        nprd = np.random.RandomState()\n",
    "        initial_sample = None\n",
    "\n",
    "        if self.targeted:\n",
    "            # Attack satisfied\n",
    "            if y == y_p:\n",
    "                return None\n",
    "\n",
    "            # Attack unsatisfied yet and the initial image satisfied\n",
    "            if adv_init is not None and init_pred == y:\n",
    "                return adv_init.astype(ART_NUMPY_DTYPE), init_pred\n",
    "\n",
    "            # Attack unsatisfied yet and the initial image unsatisfied\n",
    "            for _ in range(self.init_size):\n",
    "                random_img = nprd.uniform(clip_min, clip_max, size=x.shape).astype(x.dtype)\n",
    "                random_class = np.argmax(\n",
    "                    self.estimator.predict(np.array([random_img]), batch_size=self.batch_size),\n",
    "                    axis=1,\n",
    "                )[0]\n",
    "\n",
    "                if random_class == y:\n",
    "                    initial_sample = random_img, random_class\n",
    "\n",
    "                    logger.info(\"Found initial adversarial image for targeted attack.\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(\"Failed to draw a random image that is adversarial, attack failed.\")\n",
    "\n",
    "        else:\n",
    "            # The initial image satisfied\n",
    "            if adv_init is not None and init_pred != y_p:\n",
    "                return adv_init.astype(ART_NUMPY_DTYPE), init_pred\n",
    "\n",
    "            # The initial image unsatisfied\n",
    "            for _ in range(self.init_size):\n",
    "                random_img = nprd.uniform(clip_min, clip_max, size=x.shape).astype(x.dtype)\n",
    "                random_class = np.argmax(\n",
    "                    self.estimator.predict(np.array([random_img]), batch_size=self.batch_size),\n",
    "                    axis=1,\n",
    "                )[0]\n",
    "\n",
    "                if random_class != y_p:\n",
    "                    initial_sample = random_img, random_class\n",
    "\n",
    "                    logger.info(\"Found initial adversarial image for untargeted attack.\")\n",
    "                    break\n",
    "            else:  # pragma: no cover\n",
    "                logger.warning(\"Failed to draw a random image that is adversarial, attack failed.\")\n",
    "\n",
    "        return initial_sample\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_adv(original_sample: np.ndarray, potential_advs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        From the potential adversarial examples, find the one that has the minimum L2 distance from the original sample\n",
    "        :param original_sample: The original input.\n",
    "        :param potential_advs: Array containing the potential adversarial examples\n",
    "        :return: The adversarial example that has the minimum L2 distance from the original input\n",
    "        \"\"\"\n",
    "        shape = potential_advs.shape\n",
    "        min_idx = np.linalg.norm(original_sample.flatten() - potential_advs.reshape(shape[0], -1), axis=1).argmin()\n",
    "        return potential_advs[min_idx]\n",
    "\n",
    "    def _check_params(self) -> None:\n",
    "        if not isinstance(self.max_iter, int) or self.max_iter < 0:\n",
    "            raise ValueError(\"The number of iterations must be a non-negative integer.\")\n",
    "\n",
    "        if not isinstance(self.num_trial, int) or self.num_trial < 0:\n",
    "            raise ValueError(\"The number of trials must be a non-negative integer.\")\n",
    "\n",
    "        if not isinstance(self.sample_size, int) or self.sample_size <= 0:\n",
    "            raise ValueError(\"The number of samples must be a positive integer.\")\n",
    "\n",
    "        if not isinstance(self.init_size, int) or self.init_size <= 0:\n",
    "            raise ValueError(\"The number of initial trials must be a positive integer.\")\n",
    "\n",
    "        if self.epsilon <= 0:\n",
    "            raise ValueError(\"The initial step size for the step towards the target must be positive.\")\n",
    "\n",
    "        if self.delta <= 0:\n",
    "            raise ValueError(\"The initial step size for the orthogonal step must be positive.\")\n",
    "\n",
    "        if self.step_adapt <= 0 or self.step_adapt >= 1:\n",
    "            raise ValueError(\"The adaptation factor must be in the range (0, 1).\")\n",
    "\n",
    "        if not isinstance(self.min_epsilon, (float, int)) or self.min_epsilon < 0:\n",
    "            raise ValueError(\"The minimum epsilon must be non-negative.\")\n",
    "\n",
    "        if not isinstance(self.verbose, bool):\n",
    "            raise ValueError(\"The argument `verbose` has to be of type bool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image [0.0, 1.0]\n",
    "x = np.asarray(Image.open('/home/aya/Desktop/Kitti-ObjectDetection/kitti-YOLOFormat/Adversarial.jpg').resize((32, 32))) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BoundaryAttack.__init__() missing 1 required positional argument: 'estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a\u001b[39m=\u001b[39mBoundaryAttack()\n",
      "\u001b[0;31mTypeError\u001b[0m: BoundaryAttack.__init__() missing 1 required positional argument: 'estimator'"
     ]
    }
   ],
   "source": [
    "a=BoundaryAttack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils import load_mnist\n",
    "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
