{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n",
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/convert/attribute.py:101: UserWarning: Pytorch's interpolate uses no coordinate_transformation_mode=asymmetric. Result might differ.\n",
      "  warnings.warn(\n",
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/operations/resize.py:16: UserWarning: Pytorch's interpolate uses no cubic_coeff_a. Result might differ.\n",
      "  warnings.warn(\n",
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/operations/resize.py:16: UserWarning: Pytorch's interpolate uses no nearest_mode. Result might differ.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvertModel(\n",
       "  (Conv_/model.0/conv/Conv_output_0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.1/conv/Conv_output_0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.2/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.2/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_140): Constant(constant=tensor([16, 16]))\n",
       "  (Split_/model.2/Split_output_0): Split()\n",
       "  (Conv_/model.2/m.0/cv1/conv/Conv_output_0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.2/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.2/m.0/cv2/conv/Conv_output_0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.2/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.2/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.2/cv2/conv/Conv_output_0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.2/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.3/conv/Conv_output_0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.3/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.3/act/Mul_output_0): mul()\n",
       "  (Conv_/model.4/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.4/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_160): Constant(constant=tensor([32, 32]))\n",
       "  (Split_/model.4/Split_output_0): Split()\n",
       "  (Conv_/model.4/m.0/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.4/m.0/cv2/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.4/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.4/m.1/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.1/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.1/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.4/m.1/cv2/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.1/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.1/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.4/m.1/Add_output_0): Add()\n",
       "  (Conv_/model.4/cv2/conv/Conv_output_0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.4/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.5/conv/Conv_output_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.5/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.5/act/Mul_output_0): mul()\n",
       "  (Conv_/model.6/cv1/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.6/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_187): Constant(constant=tensor([64, 64]))\n",
       "  (Split_/model.6/Split_output_0): Split()\n",
       "  (Conv_/model.6/m.0/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.6/m.0/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.6/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.6/m.1/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.1/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.1/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.6/m.1/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.1/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.1/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.6/m.1/Add_output_0): Add()\n",
       "  (Conv_/model.6/cv2/conv/Conv_output_0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.6/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.7/conv/Conv_output_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.7/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.7/act/Mul_output_0): mul()\n",
       "  (Conv_/model.8/cv1/conv/Conv_output_0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.8/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_214): Constant(constant=tensor([128, 128]))\n",
       "  (Split_/model.8/Split_output_0): Split()\n",
       "  (Conv_/model.8/m.0/cv1/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.8/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.8/m.0/cv2/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.8/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.8/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.8/cv2/conv/Conv_output_0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.8/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.9/cv1/conv/Conv_output_0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.9/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.9/cv1/act/Mul_output_0): mul()\n",
       "  (MaxPool_/model.9/m/MaxPool_output_0): MaxPool2d(kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=1, ceil_mode=False)\n",
       "  (MaxPool_/model.9/m_1/MaxPool_output_0): MaxPool2d(kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=1, ceil_mode=False)\n",
       "  (MaxPool_/model.9/m_2/MaxPool_output_0): MaxPool2d(kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=1, ceil_mode=False)\n",
       "  (Conv_/model.9/cv2/conv/Conv_output_0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.9/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.9/cv2/act/Mul_output_0): mul()\n",
       "  (Constant_/model.10/Constant_output_0): Constant(constant=tensor([1., 1., 2., 2.]))\n",
       "  (Resize_/model.10/Resize_output_0): Resize()\n",
       "  (Conv_/model.12/cv1/conv/Conv_output_0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.12/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.12/Split_output_0): Split()\n",
       "  (Conv_/model.12/m.0/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.12/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.12/m.0/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.12/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.12/cv2/conv/Conv_output_0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.12/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/cv2/act/Mul_output_0): mul()\n",
       "  (Constant_/model.13/Constant_output_0): Constant(constant=tensor([1., 1., 2., 2.]))\n",
       "  (Resize_/model.13/Resize_output_0): Resize()\n",
       "  (Conv_/model.15/cv1/conv/Conv_output_0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.15/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.15/Split_output_0): Split()\n",
       "  (Conv_/model.15/m.0/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.15/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.15/m.0/cv2/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.15/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.15/cv2/conv/Conv_output_0): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.15/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.16/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.16/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.16/act/Mul_output_0): mul()\n",
       "  (Conv_/model.18/cv1/conv/Conv_output_0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.18/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.18/Split_output_0): Split()\n",
       "  (Conv_/model.18/m.0/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.18/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.18/m.0/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.18/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.18/cv2/conv/Conv_output_0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.18/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.19/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.19/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.19/act/Mul_output_0): mul()\n",
       "  (Conv_/model.21/cv1/conv/Conv_output_0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.21/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.21/Split_output_0): Split()\n",
       "  (Conv_/model.21/m.0/cv1/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.21/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.21/m.0/cv2/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.21/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.21/cv2/conv/Conv_output_0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.21/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.0/cv2.0.0/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.0/cv2.0.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.0/cv2.0.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.0/cv2.0.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.0/cv2.0.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.0/cv2.0.2/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.0/cv3.0.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.0/cv3.0.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.0/cv3.0.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.0/cv3.0.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.0/cv3.0.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.0/cv3.0.2/Conv_output_0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv2.1/cv2.1.0/conv/Conv_output_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.1/cv2.1.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.1/cv2.1.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.1/cv2.1.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.1/cv2.1.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.1/cv2.1.2/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv3.1/cv3.1.0/conv/Conv_output_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.1/cv3.1.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.1/cv3.1.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.1/cv3.1.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.1/cv3.1.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.1/cv3.1.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.1/cv3.1.2/Conv_output_0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv2.2/cv2.2.0/conv/Conv_output_0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.2/cv2.2.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.2/cv2.2.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.2/cv2.2.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.2/cv2.2.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.2/cv2.2.2/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv3.2/cv3.2.0/conv/Conv_output_0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.2/cv3.2.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.2/cv3.2.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.2/cv3.2.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.2/cv3.2.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.2/cv3.2.2/Conv_output_0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Constant_/model.22/Constant_output_0): Constant(constant=tensor([ 1, 73, -1]))\n",
       "  (Constant_/model.22/Constant_1_output_0): Constant(constant=tensor([ 1, 73, -1]))\n",
       "  (Constant_/model.22/Constant_2_output_0): Constant(constant=tensor([ 1, 73, -1]))\n",
       "  (Reshape_/model.22/Reshape_output_0): Reshape(shape=None)\n",
       "  (Reshape_/model.22/Reshape_1_output_0): Reshape(shape=None)\n",
       "  (Reshape_/model.22/Reshape_2_output_0): Reshape(shape=None)\n",
       "  (Constant_onnx::Split_391): Constant(constant=tensor([64,  9]))\n",
       "  (Split_/model.22/Split_output_0): Split()\n",
       "  (Constant_/model.22/dfl/Constant_output_0): Constant(constant=tensor([    1,     4,    16, 31941]))\n",
       "  (Reshape_/model.22/dfl/Reshape_output_0): Reshape(shape=None)\n",
       "  (Transpose_/model.22/dfl/Transpose_output_0): Transpose()\n",
       "  (Softmax_/model.22/dfl/Softmax_output_0): Softmax(dim=1)\n",
       "  (Conv_/model.22/dfl/conv/Conv_output_0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (Constant_/model.22/dfl/Constant_1_output_0): Constant(constant=tensor([    1,     4, 31941]))\n",
       "  (Reshape_/model.22/dfl/Reshape_1_output_0): Reshape(shape=None)\n",
       "  (Constant_/model.22/Constant_3_output_0): Constant(constant=tensor([2, 2]))\n",
       "  (Split_/model.22/Split_1_output_0): Split()\n",
       "  (Constant_/model.22/Constant_4_output_0): Constant(\n",
       "    constant=tensor([[[ 0.5000,  1.5000,  2.5000,  ..., 36.5000, 37.5000, 38.5000],\n",
       "             [ 0.5000,  0.5000,  0.5000,  ..., 38.5000, 38.5000, 38.5000]]])\n",
       "  )\n",
       "  (Sub_/model.22/Sub_output_0): mul()\n",
       "  (Constant_/model.22/Constant_5_output_0): Constant(\n",
       "    constant=tensor([[[ 0.5000,  1.5000,  2.5000,  ..., 36.5000, 37.5000, 38.5000],\n",
       "             [ 0.5000,  0.5000,  0.5000,  ..., 38.5000, 38.5000, 38.5000]]])\n",
       "  )\n",
       "  (Add_/model.22/Add_output_0): Add()\n",
       "  (Add_/model.22/Add_1_output_0): Add()\n",
       "  (Constant_/model.22/Constant_6_output_0): Constant(constant=2.0)\n",
       "  (Div_/model.22/Div_output_0): Div()\n",
       "  (Sub_/model.22/Sub_1_output_0): mul()\n",
       "  (Constant_/model.22/Constant_7_output_0): Constant(constant=tensor([[ 8.,  8.,  8.,  ..., 32., 32., 32.]]))\n",
       "  (Mul_/model.22/Mul_output_0): mul()\n",
       "  (Sigmoid_/model.22/Sigmoid_output_0): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert onnx to pytorch\n",
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "onnx_model = onnx.load('/home/aya/Desktop/Kitti-ObjectDetection/best.onnx')\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "#put the model on evaluation mode\n",
    "model=pytorch_model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "\n",
    "from art.estimators.object_detection.pytorch_yolo import PyTorchYolo\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "pretrained_model = \"data/lenet_mnist_model.pth\"\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "x_train=[]\n",
    "path=\"/home/aya/Desktop/Kitti-ObjectDetection/kitti-YOLOFormat/train/images/\"\n",
    "for j in os.listdir(path):\n",
    "    img=cv2.imread(path+j)\n",
    "    img=cv2.resize(img,dsize=(375, 1244))\n",
    "    x=img = np.asarray(img)\n",
    "    x_train.append(x)\n",
    "\n",
    "y_train=[]\n",
    "path=\"/home/aya/Desktop/Kitti-ObjectDetection/kitti-YOLOFormat/train/labels/\"\n",
    "for j in os.listdir(path):\n",
    "    y=np.loadtxt(path+j)\n",
    "    y_train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch-kitti in /home/aya/.local/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/aya/.local/lib/python3.10/site-packages (from torch-kitti) (1.13.1)\n",
      "Requirement already satisfied: torchvision>=0.6.0 in /home/aya/.local/lib/python3.10/site-packages (from torch-kitti) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.51.0 in /home/aya/.local/lib/python3.10/site-packages (from torch-kitti) (4.65.0)\n",
      "Requirement already satisfied: Pillow>=7.1.0 in /usr/lib/python3/dist-packages (from torch-kitti) (9.0.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in /home/aya/.local/lib/python3.10/site-packages (from torch-kitti) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=46.3.0 in /home/aya/.local/lib/python3.10/site-packages (from torch-kitti) (67.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aya/.local/lib/python3.10/site-packages (from requests>=2.24.0->torch-kitti) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aya/.local/lib/python3.10/site-packages (from requests>=2.24.0->torch-kitti) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aya/.local/lib/python3.10/site-packages (from requests>=2.24.0->torch-kitti) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/.local/lib/python3.10/site-packages (from requests>=2.24.0->torch-kitti) (2022.12.7)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/aya/.local/lib/python3.10/site-packages (from torch>=1.5.0->torch-kitti) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/aya/.local/lib/python3.10/site-packages (from torch>=1.5.0->torch-kitti) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/aya/.local/lib/python3.10/site-packages (from torch>=1.5.0->torch-kitti) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/aya/.local/lib/python3.10/site-packages (from torch>=1.5.0->torch-kitti) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/aya/.local/lib/python3.10/site-packages (from torch>=1.5.0->torch-kitti) (4.5.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch-kitti) (0.37.1)\n",
      "Requirement already satisfied: numpy in /home/aya/.local/lib/python3.10/site-packages (from torchvision>=0.6.0->torch-kitti) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-kitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor\n",
    "\n",
    "from torch_kitti.depth_completion import KittiDepthCompletionDataset\n",
    "from torch_kitti.transforms import ApplyToFeatures\n",
    "\n",
    "class SeqKittiDepthCompletionDataset(KittiDepthCompletionDataset):\n",
    "    def __init__(self, drive_name: str, *args, **kwargs):\n",
    "        super().__init__(*args, subset=\"all\", **kwargs)\n",
    "        self.elems = sorted(\n",
    "            filter(lambda group: group.drive == drive_name and group.cam == 2, self.elems),\n",
    "            key=lambda group: group.idx\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:missing data_depth_selection.zip: folder test_depth_completion_anonymous not found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "path kitti_depth_completion_root contains wrong data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[39m=\u001b[39m ApplyToFeatures(\n\u001b[1;32m      2\u001b[0m     Compose(\n\u001b[1;32m      3\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     features\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m ds \u001b[39m=\u001b[39m KittiDepthCompletionDataset(\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mkitti_raw_sync_rect_root\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mkitti_depth_completion_root\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     load_stereo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m     load_sequence\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m     17\u001b[0m     download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# download if not found\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_kitti/depth_completion/dataset.py:218\u001b[0m, in \u001b[0;36mKittiDepthCompletionDataset.__init__\u001b[0;34m(self, kitti_raw_root, kitti_completion_root, subset, load_stereo, load_previous, load_sequence, transform, download)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m# check folders\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kitti_depth_completion_folders_check(kitti_completion_root):\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpath \u001b[39m\u001b[39m{\u001b[39;00mkitti_completion_root\u001b[39m}\u001b[39;00m\u001b[39m contains wrong data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kitti_raw_check_drives(kitti_raw_root):\n\u001b[1;32m    220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpath \u001b[39m\u001b[39m{\u001b[39;00mkitti_raw_root\u001b[39m}\u001b[39;00m\u001b[39m contains wrong data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: path kitti_depth_completion_root contains wrong data"
     ]
    }
   ],
   "source": [
    "transform = ApplyToFeatures(\n",
    "    Compose(\n",
    "        [\n",
    "            ToTensor(),\n",
    "            RandomCrop([256, 512]),\n",
    "        ]\n",
    "    ),\n",
    "    features=[\"img\"],\n",
    ")\n",
    "\n",
    "ds = KittiDepthCompletionDataset(\n",
    "    \"kitti_raw_sync_rect_root\",\n",
    "    \"kitti_depth_completion_root\",\n",
    "    load_stereo=False,\n",
    "    load_sequence=3,\n",
    "    transform=transform,\n",
    "    download=True,  # download if not found\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip to /home/aya/Kitti-ObjectDetection/KITTI-original/2D-objects/letf/testing/image_2/Kitti/raw/data_object_image_2.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f1612a315e49ff9c0eaa45d4be4a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12569945557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KITTI Test dataset and dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.Kitti('/home/aya/Kitti-ObjectDetection/KITTI-original/2D-objects/letf/testing/image_2/', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(), ])), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Run test for each epsilon\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m eps \u001b[39min\u001b[39;00m epsilons:\n\u001b[0;32m----> 7\u001b[0m     acc, ex \u001b[39m=\u001b[39m test(model, device, test_loader, eps)\n\u001b[1;32m      8\u001b[0m     accuracies\u001b[39m.\u001b[39mappend(acc)\n\u001b[1;32m      9\u001b[0m     examples\u001b[39m.\u001b[39mappend(ex)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "#run the attack\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import PyTorchClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a: Define the loss function and the optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the ART classifier\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=pytorch_model,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 1244, 375),\n",
    "    nb_classes=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the ART classifier\n",
    "classifier.fit(x_train, y_train, batch_size=8, nb_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 1242, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244, 375, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image [0.0, 1.0], resize it then normalize it\n",
    "x = np.asarray(Image.open('/home/aya/Desktop/Kitti-ObjectDetection/kitti-YOLOFormat/Adversarial.jpg').resize((32, 32))) / 255.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
