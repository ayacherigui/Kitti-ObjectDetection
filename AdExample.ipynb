{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "\n",
    "from art.estimators.object_detection.pytorch_yolo import PyTorchYolo\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n",
    "#       see, https://github.com/pytorch/vision/issues/3497 for more information\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/convert/attribute.py:101: UserWarning: Pytorch's interpolate uses no coordinate_transformation_mode=asymmetric. Result might differ.\n",
      "  warnings.warn(\n",
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/operations/resize.py:16: UserWarning: Pytorch's interpolate uses no cubic_coeff_a. Result might differ.\n",
      "  warnings.warn(\n",
      "/home/aya/.local/lib/python3.10/site-packages/onnx2pytorch/operations/resize.py:16: UserWarning: Pytorch's interpolate uses no nearest_mode. Result might differ.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvertModel(\n",
       "  (Conv_/model.0/conv/Conv_output_0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.1/conv/Conv_output_0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.2/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.2/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_140): Constant(constant=tensor([16, 16]))\n",
       "  (Split_/model.2/Split_output_0): Split()\n",
       "  (Conv_/model.2/m.0/cv1/conv/Conv_output_0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.2/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.2/m.0/cv2/conv/Conv_output_0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.2/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.2/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.2/cv2/conv/Conv_output_0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.2/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.2/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.3/conv/Conv_output_0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.3/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.3/act/Mul_output_0): mul()\n",
       "  (Conv_/model.4/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.4/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_160): Constant(constant=tensor([32, 32]))\n",
       "  (Split_/model.4/Split_output_0): Split()\n",
       "  (Conv_/model.4/m.0/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.4/m.0/cv2/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.4/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.4/m.1/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.1/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.1/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.4/m.1/cv2/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.4/m.1/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/m.1/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.4/m.1/Add_output_0): Add()\n",
       "  (Conv_/model.4/cv2/conv/Conv_output_0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.4/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.4/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.5/conv/Conv_output_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.5/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.5/act/Mul_output_0): mul()\n",
       "  (Conv_/model.6/cv1/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.6/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_187): Constant(constant=tensor([64, 64]))\n",
       "  (Split_/model.6/Split_output_0): Split()\n",
       "  (Conv_/model.6/m.0/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.6/m.0/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.6/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.6/m.1/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.1/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.1/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.6/m.1/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.6/m.1/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/m.1/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.6/m.1/Add_output_0): Add()\n",
       "  (Conv_/model.6/cv2/conv/Conv_output_0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.6/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.6/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.7/conv/Conv_output_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.7/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.7/act/Mul_output_0): mul()\n",
       "  (Conv_/model.8/cv1/conv/Conv_output_0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.8/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/cv1/act/Mul_output_0): mul()\n",
       "  (Constant_onnx::Split_214): Constant(constant=tensor([128, 128]))\n",
       "  (Split_/model.8/Split_output_0): Split()\n",
       "  (Conv_/model.8/m.0/cv1/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.8/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.8/m.0/cv2/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.8/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Add_/model.8/m.0/Add_output_0): Add()\n",
       "  (Conv_/model.8/cv2/conv/Conv_output_0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.8/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.8/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.9/cv1/conv/Conv_output_0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.9/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.9/cv1/act/Mul_output_0): mul()\n",
       "  (MaxPool_/model.9/m/MaxPool_output_0): MaxPool2d(kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=1, ceil_mode=False)\n",
       "  (MaxPool_/model.9/m_1/MaxPool_output_0): MaxPool2d(kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=1, ceil_mode=False)\n",
       "  (MaxPool_/model.9/m_2/MaxPool_output_0): MaxPool2d(kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), dilation=1, ceil_mode=False)\n",
       "  (Conv_/model.9/cv2/conv/Conv_output_0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.9/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.9/cv2/act/Mul_output_0): mul()\n",
       "  (Constant_/model.10/Constant_output_0): Constant(constant=tensor([1., 1., 2., 2.]))\n",
       "  (Resize_/model.10/Resize_output_0): Resize()\n",
       "  (Conv_/model.12/cv1/conv/Conv_output_0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.12/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.12/Split_output_0): Split()\n",
       "  (Conv_/model.12/m.0/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.12/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.12/m.0/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.12/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.12/cv2/conv/Conv_output_0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.12/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.12/cv2/act/Mul_output_0): mul()\n",
       "  (Constant_/model.13/Constant_output_0): Constant(constant=tensor([1., 1., 2., 2.]))\n",
       "  (Resize_/model.13/Resize_output_0): Resize()\n",
       "  (Conv_/model.15/cv1/conv/Conv_output_0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.15/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.15/Split_output_0): Split()\n",
       "  (Conv_/model.15/m.0/cv1/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.15/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.15/m.0/cv2/conv/Conv_output_0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.15/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.15/cv2/conv/Conv_output_0): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.15/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.15/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.16/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.16/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.16/act/Mul_output_0): mul()\n",
       "  (Conv_/model.18/cv1/conv/Conv_output_0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.18/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.18/Split_output_0): Split()\n",
       "  (Conv_/model.18/m.0/cv1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.18/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.18/m.0/cv2/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.18/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.18/cv2/conv/Conv_output_0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.18/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.18/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.19/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (Sigmoid_/model.19/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.19/act/Mul_output_0): mul()\n",
       "  (Conv_/model.21/cv1/conv/Conv_output_0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.21/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/cv1/act/Mul_output_0): mul()\n",
       "  (Split_/model.21/Split_output_0): Split()\n",
       "  (Conv_/model.21/m.0/cv1/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.21/m.0/cv1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/m.0/cv1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.21/m.0/cv2/conv/Conv_output_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.21/m.0/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/m.0/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.21/cv2/conv/Conv_output_0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Sigmoid_/model.21/cv2/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.21/cv2/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.0/cv2.0.0/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.0/cv2.0.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.0/cv2.0.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.0/cv2.0.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.0/cv2.0.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.0/cv2.0.2/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.0/cv3.0.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.0/cv3.0.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.0/cv3.0.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.0/cv3.0.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.0/cv3.0.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.0/cv3.0.2/Conv_output_0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv2.1/cv2.1.0/conv/Conv_output_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.1/cv2.1.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.1/cv2.1.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.1/cv2.1.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.1/cv2.1.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.1/cv2.1.2/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv3.1/cv3.1.0/conv/Conv_output_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.1/cv3.1.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.1/cv3.1.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.1/cv3.1.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.1/cv3.1.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.1/cv3.1.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.1/cv3.1.2/Conv_output_0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv2.2/cv2.2.0/conv/Conv_output_0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.2/cv2.2.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.2/cv2.2.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv2.2/cv2.2.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv2.2/cv2.2.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv2.2/cv2.2.2/Conv_output_0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Conv_/model.22/cv3.2/cv3.2.0/conv/Conv_output_0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.2/cv3.2.0/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.2/cv3.2.0/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (Sigmoid_/model.22/cv3.2/cv3.2.1/act/Sigmoid_output_0): Sigmoid()\n",
       "  (Mul_/model.22/cv3.2/cv3.2.1/act/Mul_output_0): mul()\n",
       "  (Conv_/model.22/cv3.2/cv3.2.2/Conv_output_0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Constant_/model.22/Constant_output_0): Constant(constant=tensor([ 1, 73, -1]))\n",
       "  (Constant_/model.22/Constant_1_output_0): Constant(constant=tensor([ 1, 73, -1]))\n",
       "  (Constant_/model.22/Constant_2_output_0): Constant(constant=tensor([ 1, 73, -1]))\n",
       "  (Reshape_/model.22/Reshape_output_0): Reshape(shape=None)\n",
       "  (Reshape_/model.22/Reshape_1_output_0): Reshape(shape=None)\n",
       "  (Reshape_/model.22/Reshape_2_output_0): Reshape(shape=None)\n",
       "  (Constant_onnx::Split_391): Constant(constant=tensor([64,  9]))\n",
       "  (Split_/model.22/Split_output_0): Split()\n",
       "  (Constant_/model.22/dfl/Constant_output_0): Constant(constant=tensor([    1,     4,    16, 31941]))\n",
       "  (Reshape_/model.22/dfl/Reshape_output_0): Reshape(shape=None)\n",
       "  (Transpose_/model.22/dfl/Transpose_output_0): Transpose()\n",
       "  (Softmax_/model.22/dfl/Softmax_output_0): Softmax(dim=1)\n",
       "  (Conv_/model.22/dfl/conv/Conv_output_0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (Constant_/model.22/dfl/Constant_1_output_0): Constant(constant=tensor([    1,     4, 31941]))\n",
       "  (Reshape_/model.22/dfl/Reshape_1_output_0): Reshape(shape=None)\n",
       "  (Constant_/model.22/Constant_3_output_0): Constant(constant=tensor([2, 2]))\n",
       "  (Split_/model.22/Split_1_output_0): Split()\n",
       "  (Constant_/model.22/Constant_4_output_0): Constant(\n",
       "    constant=tensor([[[ 0.50000,  1.50000,  2.50000,  ..., 36.50000, 37.50000, 38.50000],\n",
       "             [ 0.50000,  0.50000,  0.50000,  ..., 38.50000, 38.50000, 38.50000]]])\n",
       "  )\n",
       "  (Sub_/model.22/Sub_output_0): mul()\n",
       "  (Constant_/model.22/Constant_5_output_0): Constant(\n",
       "    constant=tensor([[[ 0.50000,  1.50000,  2.50000,  ..., 36.50000, 37.50000, 38.50000],\n",
       "             [ 0.50000,  0.50000,  0.50000,  ..., 38.50000, 38.50000, 38.50000]]])\n",
       "  )\n",
       "  (Add_/model.22/Add_output_0): Add()\n",
       "  (Add_/model.22/Add_1_output_0): Add()\n",
       "  (Constant_/model.22/Constant_6_output_0): Constant(constant=2.0)\n",
       "  (Div_/model.22/Div_output_0): Div()\n",
       "  (Sub_/model.22/Sub_1_output_0): mul()\n",
       "  (Constant_/model.22/Constant_7_output_0): Constant(constant=tensor([[ 8.,  8.,  8.,  ..., 32., 32., 32.]]))\n",
       "  (Mul_/model.22/Mul_output_0): mul()\n",
       "  (Sigmoid_/model.22/Sigmoid_output_0): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert onnx to pytorch\n",
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "onnx_model = onnx.load('/home/aya/Desktop/Kitti-ObjectDetection/best.onnx')\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "\n",
    "#put the model in evaluation mode\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "pretrained_model = \"'/home/aya/Desktop/Kitti-ObjectDetection/yolov8n_costum.pt'\"\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomCrop, ToTensor\n",
    "\n",
    "from torch_kitti.depth_completion import KittiDepthCompletionDataset\n",
    "from torch_kitti.transforms import ApplyToFeatures\n",
    "\n",
    "transform = ApplyToFeatures(\n",
    "    Compose(\n",
    "        [\n",
    "            ToTensor(),\n",
    "            RandomCrop([256, 512]),\n",
    "        ]\n",
    "    ),\n",
    "    features=[\"img\", \"gt\", \"lidar\"],)\n",
    "\n",
    "# KITTI Test dataset and dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.Kitti('/home/aya/Kitti-ObjectDetection/KITTI-original/',\n",
    "                   train=False,\n",
    "                   download=False,\n",
    "                   transform=transform,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dict() argument after ** must be a mapping, not PngImageFile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Run test for each epsilon\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m eps \u001b[39min\u001b[39;00m epsilons:\n\u001b[0;32m----> 7\u001b[0m     acc, ex \u001b[39m=\u001b[39m test(pytorch_model, device, test_loader, eps)\n\u001b[1;32m      8\u001b[0m     accuracies\u001b[39m.\u001b[39mappend(acc)\n\u001b[1;32m      9\u001b[0m     examples\u001b[39m.\u001b[39mappend(ex)\n",
      "Cell \u001b[0;32mIn[88], line 9\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, epsilon)\u001b[0m\n\u001b[1;32m      6\u001b[0m adv_examples \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39m# Loop over all examples in test set\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[39m# Send the data and label to the device\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Set requires_grad attribute of tensor. Important for Attack\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/kitti.py:108\u001b[0m, in \u001b[0;36mKitti.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    106\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_target(index) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m--> 108\u001b[0m     image, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(image, target)\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/vision.py:95\u001b[0m, in \u001b[0;36mStandardTransform.__call__\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Any, target: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_kitti/transforms/_transforms.py:47\u001b[0m, in \u001b[0;36mApplyToFeatures.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_features(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform, x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msame_rand_state)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_kitti/transforms/functional.py:55\u001b[0m, in \u001b[0;36mapply_to_features\u001b[0;34m(transform, x, features, same_rand_state)\u001b[0m\n\u001b[1;32m     52\u001b[0m     rd_state \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mgetstate()\n\u001b[1;32m     53\u001b[0m     tr_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mget_rng_state()\n\u001b[0;32m---> 55\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mx)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mTypeError\u001b[0m: dict() argument after ** must be a mapping, not PngImageFile"
     ]
    }
   ],
   "source": [
    "#run the attack\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(pytorch_model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (7,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#plot accuracy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39;49mplot(epsilons, accuracies, \u001b[39m\"\u001b[39;49m\u001b[39m*-\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39myticks(np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39m1.1\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mxticks(np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39m.35\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (7,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
